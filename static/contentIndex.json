{"index":{"slug":"index","filePath":"index.md","title":"Welcome ğŸ‘‹","links":["posts/"],"tags":[],"content":"Hello. My name is Shane McDonald. I\nlive in Jersey City NJ with my lovely wife Chao, along with our cat Milla and\nour dog Rodger. I am currently employed by Red Hat as\none of the lead engineers for Ansible.\nI created this website to serve as a place where I can capture my thoughts, and\nto have a place where I can reflect on what I learn over the years. I spend a\nlot of my free time trying to find the balance between embracing cutting edge\ntechnology and living a simple life. Lately I have been trying avoid social\nmedia and attempting to maximize the time I spend thinking my own thoughts. If you are interested in what Iâ€™ve been up to, check out some of my recent posts.\nI am a big fan of Emacs and\nFedora, and I very much believe that open source\nsoftware has made the world a better place, and I am proud to do my part in\ncontinuing that mission.\nIf you would like to connect, my email is on my\nGitHub profile.\nHave a nice day ğŸŒ"},"posts/01-my-first-post":{"slug":"posts/01-my-first-post","filePath":"posts/01-my-first-post.md","title":"My first post","links":[],"tags":[],"content":"Why I made this website\nItâ€™s been a while since I had the time or cared enough to maintain a website for\nmyself. As a software engineer, I found that over time I gradually lost interest\nin using a computer outside of work.\nThat made me sad, because I am truly grateful to be paid to do something that I\nhad at one point spent countless hours doing in my free time. After enough time\nhad passed, I began to reflect on the situation and try and start to understand\nwhat had happened. The answer seems simple now - I had stopped learning for the\nsake of learning.\nWhen acquiring new a new skill, almost every day is filled with\nfirst-encounters. Especially so with programming. The amount of information\nrequired to begin understanding how software interacts with underlying physical\nhardware is more than enough to keep you busy for multiple lifetimes. But people\ntend to achieve a level of proficiency that allows them to carry out the task at\nhand and then stop learning.\nThis is understandable - thereâ€™s only so many hours in the day and there are\nother things in life to care about. Itâ€™s largely what happened to me. Once I\nstarted making enough money to pay the bills and support my family, I felt less\nof a need to learn things outside of what it took to accomplish the immediate\ntasks at hand at work.\nWriting code for a living was enough to keep me mentally stimulated for more\nthan a decade, but once I started taking on more leadership responsibilities it\ndidnâ€™t take long before I felt my skills begin to atrophy.\nThis is something I have heard people more senior than myself talk about my\nentire career, but when it started happening to me it felt terrifying. If I lost\nmy ability to do the thing that had gotten me to this point in the first place,\nhow long until Iâ€™m no longer useful to the people Iâ€™m entrusted to lead?\nThis may all sound a bit melodramatic, but itâ€™s been gnawing at me for years\nnow. I just havenâ€™t have the time, energy, or ability to do anything to change\nit. I still love my job - and it was never really an option to go back. I found\n(and still find) it very rewarding to have more of an impact than what I could\nachieve by only writing code. However, somewhat fortuitously, the less time I\nhad to do what I truly love at work, the more time I was willing to spend my\nfree time searching for ways to reintegrate this passion into my daily life.\nGoing forward\nIâ€™ve spent the past several months rediscovering things that I had previously\nlost interest in. Simple things like building computers, installing operating\nsystems, configuring my desktop environment, and tweaking my favorite text\neditor. Itâ€™s been a lot of fun, and going forward I am going to write about them\nhere. This will mostly be a form of self-therapy and archiving for the sake of\nposterity, but Iâ€™m going to do this publicly in the off chance that someone else\nfinds anything I have to say interesting or useful. Thatâ€™s it for now. Thank you\nif youâ€™ve decided to read this far, and have a nice day."},"posts/02-how-i-made-this-website":{"slug":"posts/02-how-i-made-this-website","filePath":"posts/02-how-i-made-this-website.md","title":"How I made this website","links":["posts/01-my-first-post"],"tags":[],"content":"In my last post (which also happened to be My first post) I provided a little bit of context as to why I made this website. Now that Iâ€™ve gotten that over with, Iâ€™m going to continue on my merry way and pontificate on random and mostly technical things that I have been occupying my time. I figured a good place to start would be to capture how I made this website.\nMarkdown, please\nAs Iâ€™ve gotten older, Iâ€™ve begun to care more about retaining full control of my data. Iâ€™ve also developed more of a propensity towards simplicity, and that involves searching out things that Iâ€™m able to understand and easily navigate. Tools that just get out of my way. At this point, writing in Markdown has become second nature. I find myself using it even I donâ€™t mean to.\nObsidian\nI think most developers have heard of Obsidian by now - an application for writing and managing content in Markdown. Anyone who has written a lot of Markdown-based content will know that it can become unwieldy to manage over time, especially when linking between documents and using some more of its advanced features like tables. Obsidian makes these things easy, with real-time previews and the ability to automatically update links when you move things around.\nQuartz\nWhile Obsidian has its own (paid) option to publish to a website hosted on their servers, it is after all just a directory full of files. While there are several static website generators out there that support Markdown, none are quite as polished as Quartz- which also happens to be Obsidian compatible out of the box. If you havenâ€™t seen Quartz before, I could not recommend it enough - itâ€™s fast, has an extremely powerful search feature, live reloading, and is pretty much infinitely flexible if youâ€™re willing to spend a little time learning TypeScript.\nRunning Quartz locally\nAs much as I love Quartz, I did not love how their documentation currently instructs users to clone the repository locally, install NodeJS, and store content within a fork of the Quartz project.\nCustom tooling\nTo prevent installing Node on my computer, I put together a custom Containerfile that installs the necessary dependencies and clones the Quartz repo:\nFROM registry.fedoraproject.org/fedora\n \nARG QUARTZ_REF=eccad3da5d7b84b0f78a85b357efedef8c0127fc\n \nUSER root\n \nRUN dnf install -y git make nodejs &amp;&amp; \\\n    npm install -g n &amp;&amp; \\\n    n lts &amp;&amp; \\\n    npm install -g npm@latest &amp;&amp; \\\n    dnf remove -y nodejs\n \nRUN git config --global --add safe.directory /repo\nRUN cd /opt &amp;&amp; git clone github.com/jackyzha0/quartz.git &amp;&amp; \\\n    cd quartz &amp;&amp; git checkout ${QUARTZ_REF} &amp;&amp; \\\n    npm ci\n \nCOPY quartz.config.ts /opt/quartz/\nCOPY quartz.layout.ts /opt/quartz/\n \nWORKDIR /opt/quartz\nNote here how I clone Quartz into the image and only COPY the files that I need to customize.\nWhy am I currently using a random SHA rather than a tagged version? While I was in the process of building out this website I came across this issue - although they didnâ€™t end up accepting my PR, Iâ€™m grateful that the maintainers of Quartz were willing to collaborate and fix the underlying problem.\nMake it simple\nSome things are just better off not being rewritten in JavaScript. I think Make is one of these things. It might have first came out closer to the Moon landing than when I built this website, but itâ€™s still more powerful than anything thatâ€™s been written in recent years and itâ€™s way less likely to be abandoned as the project some dude wrote over the weekend and shared on Hacker News.\nSee the full version of my Makefile for this website for more information, but the gist of it is here:\n.DEFAULT_GOAL := scratchpad\n \n.PHONY: scratchpad\nscratchpad: check_image_uptodate\n\t@$(CONTAINER_RUNTIME) run --rm -ti \\\n\t\t-p $(QUARTZ_SERVER_PORT):$(QUARTZ_SERVER_PORT) \\\n\t\t-p $(QUARTZ_WEBSOCKET_PORT):$(QUARTZ_WEBSOCKET_PORT) \\\n\t\t$(COMMON_MOUNTS) \\\n\t\t$(SCRATCHPAD_IMAGE_NAME) \\\n \n.PHONY: check_image_uptodate\ncheck_image_uptodate: image\n\t@echo &quot;Checking current image SHA...&quot;\n\t@current_sha=$$($(CONTAINER_RUNTIME) inspect --format=&#039;{{.Id}}&#039; $(SCRATCHPAD_IMAGE_NAME):$(IMAGE_TAG) 2&gt;/dev/null || echo &quot;none&quot;); \\\n\tstored_sha=$$(cat $(IMAGE_ID_FILE) 2&gt;/dev/null || echo &quot;none&quot;); \\\n\tif [ &quot;$$current_sha&quot; != &quot;$$stored_sha&quot; ] || [ -z &quot;$$current_sha&quot; ]; then \\\n\t\techo &quot;Building image... $$current_sha $$stored_sha&quot;; \\\n\t\t$(MAKE) -B -f $(MAKEFILE_PATH) image; \\\n\telse \\\n\t\techo &quot;Image is up to date.&quot;; \\\n\tfi\n \n.PHONY: image\nimage: $(IMAGE_ID_FILE)\n \n$(IMAGE_ID_FILE): .generated Containerfile quartz.config.ts quartz.layout.ts\n\t$(CONTAINER_RUNTIME) build -t $(SCRATCHPAD_IMAGE_NAME):$(IMAGE_TAG) .\n\t$(CONTAINER_RUNTIME) inspect --format=&#039;{{.Id}}&#039; $(SCRATCHPAD_IMAGE_NAME):$(IMAGE_TAG) &gt; $(IMAGE_ID_FILE)\nThe result here is that on a machine with just make and podman (or docker) installed, I can simply clone this repo and run the command:\nâœ  shanemcd.github.io git:(main) âœ— make\nChecking current image SHA...\nImage is up to date.\nnpx quartz build --serve --port=6006 --directory=/repo/content\n\n Quartz v4.5.0\n\nCleaned output directory `public` in 2ms\nFound 4 input files from `/repo/content` in 5ms\nParsed 4 Markdown files in 112ms\nFiltered out 0 files in 24Î¼s\nEmitted 16 files to `public` in 71ms\nDone processing 4 files in 191ms\nStarted a Quartz server listening at http://localhost:6006\n\nThis handles building (and rebuilding) the container image, starting serving Quartz, mounting in the site content, and mapping ports for the web server and live-reload websocket.\nDeploying via GitHub Pages\nMost people are familiar with using GitHub Pages to host a Jekyll website, but it is also possible to serve up a pre-built HTML website that has been pushed to a branch. To automate this process I threw together this GitHub Action:\n# Simple workflow for deploying static content to GitHub Pages\nname: Deploy static content to Pages\n \npermissions:\n  contents: write\n \non:\n  # Runs on pushes targeting the default branch\n  push:\n    branches: [&quot;main&quot;]\n \n# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\nconcurrency:\n  group: &quot;pages&quot;\n  cancel-in-progress: false\n \nenv:\n  CONTAINER_RUNTIME: docker\n  RUN_CMD: &gt;-\n    QUARTZ_BUILD_OPTS=&#039;-o /repo/public&#039; make -f /repo/Makefile public\n \njobs:\n  # Single deploy job since we&#039;re just deploying\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n \n      - name: Build site\n        run: make run\n \n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: public\n \n      - name: Deploy ğŸš€\n        uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: public\n          branch: gh-pages\n          clean-exclude: pr-preview\n          force: false\nPreviewing changes\nI might be lazy, but I also love finding a good problem to over-engineer. I had originally done this for a project at work, but was able to reuse here without much effort.\nUnfortunately GitHub Pages does not natively support previewing changes. It seems like that will come at some point, but itâ€™s been almost 5 years and thereâ€™s still ETA. Luckily the Deploy PR Preview action will do just fine for now.\nHere is how Iâ€™m using it. The only thing Iâ€™ve changed is an additional invocation of marocchino/sticky-pull-request-comment, which I found rossjrw/pr-preview-action uses under the hood. I update the comment it posts to add a little reminder that the link will only work once the Action completes.\n---\nname: preview\n \npermissions:\n  contents: write\n  pull-requests: write\n \nconcurrency: preview-${{ github.ref }}\n \non:\n  pull_request_target:\n    types:\n      - opened\n      - reopened\n      - synchronize\n      - closed\n \nenv:\n  CONTAINER_RUNTIME: docker\n  RUN_CMD: &gt;-\n    QUARTZ_BUILD_OPTS=&#039;-o /repo/public&#039; make -f /repo/Makefile public\n \njobs:\n  preview:\n    runs-on: ubuntu-20.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          ref: refs/pull/${{ github.event.pull_request.number }}/merge\n          fetch-depth: 0\n \n      - run: |\n          git log\n \n      - name: Build site\n        if: github.event.action != &#039;closed&#039;\n        run: make run\n \n      - name: Deploy preview\n        uses: rossjrw/pr-preview-action@v1\n        with:\n          source-dir: public\n \n      - name: Update preview comment with note about being patient\n        uses: marocchino/sticky-pull-request-comment@v2\n        with:\n          header: pr-preview\n          append: true\n          message: |\n            &gt; [!IMPORTANT]\n            &gt; The link above will not work until the Pages deployment is complete. You can find this under [Actions -&gt; pages-build-deployment](github.com/andyettanotherorg/shanemcd.github.io/actions/workflows/pages/pages-build-deployment).\nWhich looks like this:\n\nThis might be overkill, but it lets me test things out without iterating directly on my main branch and helps for cases where I might not have access to Obsidian or the ability to test a site build locally.\nWrapping up\nI mostly captured this for my own posterity, but if for whatever reason you find yourself wanting to learn more please about any of this please check out the source for this website. If you have any questions or just want to say hi, my email is on my GitHub profile."},"posts/03-ollama-rootless-podman-quadlet":{"slug":"posts/03-ollama-rootless-podman-quadlet","filePath":"posts/03-ollama-rootless-podman-quadlet.md","title":"Running Ollama under Rootless Podman with Quadlet","links":[],"tags":[],"content":"I havenâ€™t seen any instances of other people running Ollama quite like this, so I thought I would share in case it proves to be useful for anyone else out there.\nFor those not familiar with Quadlet, it provides functionality that allows you to run and manage containers with systemd.\nNVIDIA GPU support\nBefore we can run Ollama inside of a container we first need to install the NVIDIA Container Toolkit as described here.\nGenerating the CDI specification file\nThe documentation here shows running this command manually. Given this will likely need to be re-ran over time and it is safe to re-invoke multiple times, I decided to wrap this up in a systemd unit that runs once every time my machine boots:\n[Unit]\nDescription=Generate NVIDIA CDI configuration\n \n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\nRemainAfterExit=yes\n \n[Install]\nWantedBy=multi-user.target\nPlace this in /etc/systemd/system/nvidia-cdi-generator.service and run these commands as root:\n$ systemctl daemon-reload\n$ systemctl enable --now nvidia-cdi-generator.service\n\nOllama with Podman and Quadlet\nNow that we can talk to our NVIDIA GPU from within a container, we can create a Quadlet in ~/.config/containers/systemd/ollama.container:\n[Unit]\nDescription=My Llama\nRequires=nvidia-cdi-generator\nAfter=nvidia-cdi-generator\n \n[Container]\nImage=docker.io/ollama/ollama\nAutoUpdate=registry\nPodmanArgs=--privileged --gpus=all\nEnvironment=NVIDIA_VISIBLE_DEVICES=all\nVolume=%h/.ollama:/root/.ollama\nPublishPort=11434:11434\n \n[Service]\nRestart=always\n \n[Install]\nWantedBy=default.target\nStart it by:\n$ systemctl --user daemon-reload\n$ systemctl --user start ollama.service\n\nVerify itâ€™s running by viewing the logs:\n$ journalctl --user -xeu ollama\n"},"posts/04-jetkvm-tailscale":{"slug":"posts/04-jetkvm-tailscale","filePath":"posts/04-jetkvm-tailscale.md","title":"Getting Tailscale to work on my JetKVM","links":[],"tags":[],"content":"Earlier this week I got my JetKVM in the mail. Thereâ€™s plenty of posts out there about how awesome it is, so I wonâ€™t bother to write another post reiterating that. Well, maybe briefly: it is in fact awesome. You should by one.\nRemotely accessing my JetKVM\nWhile the folks behind this product seem smart enough, Iâ€™m always skeptical about using new cloud services, especially when they have a direct line to my PC. Rather than use their Remote Access feature, I was happy to see that they also had a link in their FAQ pointing to an article on Medium called Installing Tailscale on JetKVM. This unfortunately did not work for me without some minor tweaks.\nGetting Tailscale to start automatically\nBecause JetKVM is built on top of busybox (for better or worse - worse IMHO), it lacks a modern init system. Other than being a good introduction for less experienced folks into how things used to be before systemd, it is a bit of a pain to work with.\nAnyway, after using the script shared in the Medium post linked above, the first thing I noticed was that upon reboot my device did not rejoin my tailnet.\nAfter adding some basic logging to the init script, I was able to see that it was crashing due to the TUN device not being available:\nwgengine.NewUserspaceEngine(tun &quot;tailscale0&quot;) error: tstun.New(&quot;tailscale0&quot;): CreateTUN(&quot;tailscale0&quot;) failed; /dev/net/tun does not exist\nflushing log.\nlogger closing down\nlogtail: upload: log upload of 942 bytes compressed failed: Post &quot;log.tailscale.com/c/tailnode.log.tailscale.io/986ca5896dc4d4b0b3de7618f45bf030588040a882f53c226de82ebe42fc0c5f&quot;: context canceled\nlogtail: dial &quot;log.tailscale.com:443&quot; failed: dial tcp: lookup log.tailscale.com: operation was canceled (in 1.022s), trying bootstrap...\ngetLocalBackend error: createEngine: tstun.New(&quot;tailscale0&quot;): CreateTUN(&quot;tailscale0&quot;) failed; /dev/net/tun does not exist\n\nStrangely, simply re-running this script resolved the issue and Tailscale started fine. This led me to the realization that the tun kernel module needed to be loaded ahead of time.\nHere is my version of the init script that adds some basic logging and checks and ensures that /dev/net/tun exists before trying to start tailscaled:\n#!/bin/sh\nlog=&quot;/tmp/ts.log&quot;\ntsdir=&quot;/userdata/tailscale/tailscale_1.82.5_arm&quot;\ntspath=&quot;$tsdir/tailscale&quot;\n \necho &quot;$(date): S22tailscale script starting with arg: $1&quot; &gt;&gt; $log\n \nwait_for_tun() {\n  modprobe tun 2&gt;&gt;$log\n  for i in $(seq 1 10); do\n    [ -e /dev/net/tun ] &amp;&amp; return 0\n    echo &quot;$(date): /dev/net/tun not ready, retrying...&quot; &gt;&gt; $log\n    sleep 1\n  done\n  echo &quot;$(date): /dev/net/tun still not present after waiting&quot; &gt;&gt; $log\n  return 1\n}\n \nwait_for_network() {\n  for i in $(seq 1 10); do\n    ip route | grep default &gt;/dev/null &amp;&amp; return 0\n    echo &quot;$(date): no default route yet, retrying...&quot; &gt;&gt; $log\n    sleep 1\n  done\n  echo &quot;$(date): still no default route after waiting&quot; &gt;&gt; $log\n  return 1\n}\n \nwait_for_daemon_socket() {\n  for i in $(seq 1 20); do\n    if [ -S /var/run/tailscale/tailscaled.sock ]; then\n      echo &quot;$(date): tailscaled socket ready&quot; &gt;&gt; $log\n      return 0\n    fi\n    echo &quot;$(date): waiting for tailscaled socket...&quot; &gt;&gt; $log\n    sleep 1\n  done\n  echo &quot;$(date): tailscaled socket did not appear in time&quot; &gt;&gt; $log\n  return 1\n}\n \ncase &quot;$1&quot; in\n  start)\n    wait_for_tun || exit 1\n    wait_for_network || exit 1\n    echo &quot;$(date): Starting tailscaled...&quot; &gt;&gt; $log\n    nohup env TS_DEBUG_FIREWALL_MODE=nftables &quot;$tsdir/tailscaled&quot; \\\n      -statedir /userdata/tailscale-state \\\n      &gt;&gt; $log 2&gt;&amp;1 &lt;/dev/null &amp;\n    wait_for_daemon_socket || exit 1\n    ;;\n  stop)\n    echo &quot;$(date): Stopping tailscaled...&quot; &gt;&gt; $log\n    killall tailscaled &gt;&gt; $log 2&gt;&amp;1\n    ;;\n  *)\n    echo &quot;Usage: $0 {start|stop}&quot; &gt;&amp;2\n    exit 1\n    ;;\nesac\nImportant: Use nohup for proper daemonization\nWhen running tailscaled via SSH (like when Ansible executes the init script), the daemon must be properly detached from the controlling terminal. Using nohup with stdin redirected from /dev/null ensures the process survives when the SSH session closes:\nnohup env TS_DEBUG_FIREWALL_MODE=nftables &quot;$tsdir/tailscaled&quot; \\\n  -statedir /userdata/tailscale-state \\\n  &gt;&gt; $log 2&gt;&amp;1 &lt;/dev/null &amp;\nWithout nohup, the backgrounded process receives a SIGHUP signal when the SSH session closes, causing it to exit. This is especially important when managing the daemon through configuration management tools like Ansible.\nOrthogonal issue with non-persistent MAC address\n\n\n                  \n                  Update on May 10, 2025 \n                  \n                \n\n\nLooks like this has now been fixed.\n\n\n\nDuring the process of setting up my JetKVM, I noticed that I was getting a new IP every time the device restarted. The first thing I tried was to give in a static IP through my UniFi console, but was surprised when after a reboot I got yet another IP. This led to me finding this GitHub issuethat has a ton of activity on it. I suspect this will be fixed soon, but in the meantime, this comment had a solution that resolves the issue."},"posts/05-kind-podman-exec-as-root":{"slug":"posts/05-kind-podman-exec-as-root","filePath":"posts/05-kind-podman-exec-as-root.md","title":"Obtaining root access in a pod running under kind on Podman","links":[],"tags":[],"content":"I am using a development environment that utilizes kind running under rootless Podman. Hereâ€™s how I was able to save some time while debugging and avoid needing to rebuild/redeploy when testing changes to containers that are not running as root.\nFirst, we need to exec from our host into the kind container:\n$ podman exec -ti kind-control-plane bash\n\nNext, locate the container we want to access:\nroot@kind-control-plane:/# crictl ps | grep api\n46fc42d2ed03c Â Â Â Â Â Â 09ba385429956 Â Â Â Â Â Â 16 minutes ago Â Â Â Â Â Running Â Â Â Â Â Â Â Â Â Â Â Â api Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 8f72c70f538a5 Â Â Â Â Â Â my-app-8544786747-rzwlp Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â default\n\nObtain the full ID:\nroot@kind-control-plane:/# crictl inspect 46fc42d2ed03c | jq -r &#039;.status.id&#039;\n46fc42d2ed03c7e42452725bcdea05c089958b1d2c62f4d68526c2640e8cab8a\n\nNow we can gain root access to our container:\nroot@kind-control-plane:/# ctr --namespace k8s.io tasks exec --user 0 --exec-id debug --tty 46fc42d2ed03c7e42452725bcdea05c089958b1d2c62f4d68526c2640e8ca\nb8a /bin/sh\nsh-4.4# id -u Â Â \n0\n\nğŸ¤˜"},"posts/06-golang-debugging-on-fedora-atomic":{"slug":"posts/06-golang-debugging-on-fedora-atomic","filePath":"posts/06-golang-debugging-on-fedora-atomic.md","title":"Debuging a Golang project on a Fedora Atomic Desktop (with Emacs and Dape)","links":[],"tags":[],"content":"A couple months ago, I tried to get Dape working with a Golang project and ran into a couple of problems that prevented me from being able to use it. Today I decided to revisit it, and Iâ€™m glad I did, because it was one of the last remaining reasons I had for ever needing to open VSCode.\nFirst attempt at using Dape with a Golang project\nProblem 1 - passing arguments\nIn the top-level of my repo, I created the file .dir-locals.el with the following content:\n((go-mode . ((dape-configs .\n        ((go-debug-main\n          modes (go-mode go-ts-mode)\n          command &quot;dlv&quot;\n          command-args (&quot;dap&quot; &quot;--listen&quot; &quot;127.0.0.1:55878&quot; &quot;--log-dest&quot; &quot;/tmp/dlv.log&quot;)\n          command-cwd &quot;/home/shanemcd/github/GoogleContainerTools/skaffold/examples/simple-artifact-dependency&quot;\n          host &quot;127.0.0.1&quot;\n          port 55878\n          :request &quot;launch&quot;\n          :mode &quot;debug&quot;\n          :type &quot;go&quot;\n          :showLog &quot;true&quot;\n          :program &quot;/home/shanemcd/github/GoogleContainerTools/skaffold/cmd/skaffold/skaffold.go&quot;\n          :args (&quot;build&quot;)))))))\n \nI am using Googleâ€™s Skaffold here in my example, but any program that accepts command line arguments will serve the purpose.\nWith this config in place, I navigated to the programâ€™s entrypoint (cmd/skaffold/skaffold.go) and was prompted with this:\nThe local variables list in /home/shanemcd/github/GoogleContainerTools/skaffold/\nor .dir-locals.el contains values that may not be safe (*).\n\nDo you want to apply it?  You can type\ny  -- to apply the local variables list.\nn  -- to ignore the local variables list.\n!  -- to apply the local variables list, and permanently mark these\n      values (*) as safe (in the future, they will be set automatically.)\ni  -- to ignore the local variables list, and permanently mark these\n      values (*) as ignored\n+  -- to apply the local variables list, and trust all directory-local\n      variables in this directory\n\n  * dape-configs : ((go-debug-main modes (go-mode go-ts-mode) command &quot;dlv&quot; command-args (&quot;dap&quot; &quot;--listen&quot; &quot;127.0.0.1:55878&quot; &quot;--log-dest&quot; &quot;/tmp/dlv.log&quot;) command-cwd &quot;/var/home/shanemcd/github/GoogleContainerTools/skaffold/examples/simple-artifact-dependency&quot; host &quot;127.0.0.1&quot; port 55878 :request &quot;launch&quot; :mode &quot;debug&quot; :type &quot;go&quot; :showLog &quot;true&quot; :program &quot;/var/home/shanemcd/github/GoogleContainerTools/skaffold/cmd/skaffold/skaffold.go&quot; :args (&quot;build&quot;)))\n\nAfter typing y and pressing Return. With the file open, I then ran M-x dape and saw the following output in the *dape-repl* buffer:\n* Welcome to Dape REPL! *\nAvailable Dape commands: debug, next, continue, pause, step, out, up, down, threads, stack, modules, sources, breakpoints, scope, watch, restart, kill, disconnect, quit\nEmpty input will rerun last command.\n\n* Process launched /var/home/shanemcd/github/GoogleContainerTools/skaffold/examples/simple-artifact-dependency/__debug_bin1126826703 *\nType &#039;dlv help&#039; for list of commands.\nHello\nA tool that facilitates continuous development for Kubernetes applications.\n\n  Find more information at: skaffold.dev/docs/getting-started/\n\nEnd-to-end Pipelines:\n  run                 Run a pipeline\n  dev                 Run a pipeline in development mode\n  debug               Run a pipeline in debug mode\n\nPipeline Building Blocks:\n  build               Build the artifacts\n  test                Run tests against your built application images\n  deploy              Deploy pre-built artifacts\n  delete              Delete any resources deployed by Skaffold\n  render              Generate rendered Kubernetes manifests\n  apply               Apply hydrated manifests to a cluster\n  verify              Run verification tests against skaffold deployments\n\nGetting Started With a New Project:\n  init                Generate configuration for deploying an application\n\nOther Commands:\n  completion          Output shell completion for the given shell (bash, fish or zsh)\n  config              Interact with the global Skaffold config file (defaults to `$HOME/.skaffold/config`)\n  diagnose            Run a diagnostic on Skaffold\n  exec                Execute a custom action\n  fix                 Update old configuration to a newer schema version\n  schema              List JSON schemas used to validate skaffold.yaml configuration\n  survey              Opens a web browser to fill out the Skaffold survey\n  version             Print the version information\n\nUsage:\n  skaffold [flags] [options]\n\nUse &quot;skaffold &lt;command&gt; --help&quot; for more information about a given command.\nUse &quot;skaffold options&quot; for a list of global command-line options (applies to all commands).\n\nWell, that didnâ€™t work as I had expected.\nReading over the dape README, I saw this under the C, C++ and Rust - lldb-dap heading:\n\nTo pass arguments, use :args [&quot;arg1&quot; &quot;arg2&quot; ..]\n\nAt first I did not try this because I assumed if it also applied to other languages it would have been called out explicitly, but sure enough, it worked after running dape and then appending my args:\ngo-debug-main :args [&quot;build&quot;]\n\nI then tried to update my .dir-locals.el with the bracket notation:\ndiff --git a/.dir-locals.el b/.dir-locals.el\nindex 65e164b..9995c3a 100644\n--- a/.dir-locals.el\n+++ b/.dir-locals.el\n@@ -11,4 +11,4 @@\nÂ Â Â Â Â Â Â Â Â Â :type &quot;go&quot;\nÂ Â Â Â Â Â Â Â Â Â :showLog &quot;true&quot;\nÂ Â Â Â Â Â Â Â Â Â :program &quot;/home/shanemcd/github/GoogleContainerTools/skaffold/cmd/skaffold/skaffold.go&quot;\n- Â Â Â Â Â Â Â Â Â :args (&quot;build&quot;)))))))\n+ Â Â Â Â Â Â Â Â Â :args [&quot;build&quot;]))))))\nâ€¦ and it worked! ğŸ‰ (but alsoğŸ¤¦â€â™‚ï¸)\n* Welcome to Dape REPL! *\nAvailable Dape commands: debug, next, continue, pause, step, out, up, down, threads, stack, modules, sources, breakpoints, scope, watch, restart, kill, disconnect, quit\nEmpty input will rerun last command.\n\n* Process launched /var/home/shanemcd/github/GoogleContainerTools/skaffold/examples/simple-artifact-dependency/__debug_bin2338563526 build *\nType &#039;dlv help&#039; for list of commands.\nGenerating tags...\n - app -&gt; app:v2.15.0-4-g8c00b98d7\n - base -&gt; base:v2.15.0-4-g8c00b98d7\nChecking cache...\n - app: Not found. Building\n - base: Not found. Building\nStarting build...\nBuilding [base]...\nSending build context to Docker daemon  3.072kB\n\nStep 1/3 : FROM alpine:3\n ---&gt; aded1e1a5b37\nStep 2/3 : COPY hello.txt .\n ---&gt; Using cache\n ---&gt; 5739d7557ae4\nStep 3/3 : CMD [&quot;./app&quot;]\n ---&gt; Using cache\n ---&gt; 17979f81cffb\nSuccessfully built 17979f81cffb\nSuccessfully tagged base:v2.15.0-4-g8c00b98d7\nThe push refers to repository [docker.io/library/base]\n23deea18e759: Preparing\n08000c18d16d: Preparing\nProcess 160667 has exited with status 1\nDetaching\n* Session terminated *\n\nProblem 2 - breakpoints not working\nNow with the command invocation working, I proceeded with trying to set a breakpoint in the Skaffold code with dape-breakpoint-toggle C-x C-a b. Doing that and then re-running dape, I was surprised to see that the breakpoint did not hit as I was expecting.\nAfter describing my problem to ChatGPT, it inferred from our previous chats that I was using Fedora Kinoite, and it pointed out right away that symlinks (Fedora Atomic Desktops link /home/ to /var/home) were known to cause problem in debuggers. After a quick search, that was easy to confirm after locating this note in the vscode-go docs. I was able to get around this by setting HOME to the full path when in my .dir-locals.el\ndiff --git a/.dir-locals.el b/.dir-locals.el\nindex 9995c3a..fba8c9f 100644\n--- a/.dir-locals.el\n+++ b/.dir-locals.el\n@@ -3,12 +3,12 @@\nÂ Â Â Â Â Â Â Â Â Â modes (go-mode go-ts-mode)\nÂ Â Â Â Â Â Â Â Â Â command &quot;dlv&quot;\nÂ Â Â Â Â Â Â Â Â Â command-args (&quot;dap&quot; &quot;--listen&quot; &quot;127.0.0.1:55878&quot; &quot;--log-dest&quot; &quot;/tmp/dlv.log&quot;)\n- Â Â Â Â Â Â Â Â Â command-cwd &quot;/home/shanemcd/github/GoogleContainerTools/skaffold/examples/simple-artifact-dependency&quot;\n+ Â Â Â Â Â Â Â Â Â command-cwd &quot;/var/home/shanemcd/github/GoogleContainerTools/skaffold/examples/simple-artifact-dependency&quot;\nÂ Â Â Â Â Â Â Â Â Â host &quot;127.0.0.1&quot;\nÂ Â Â Â Â Â Â Â Â Â port 55878\nÂ Â Â Â Â Â Â Â Â Â :request &quot;launch&quot;\nÂ Â Â Â Â Â Â Â Â Â :mode &quot;debug&quot;\nÂ Â Â Â Â Â Â Â Â Â :type &quot;go&quot;\nÂ Â Â Â Â Â Â Â Â Â :showLog &quot;true&quot;\n- Â Â Â Â Â Â Â Â Â :program &quot;/home/shanemcd/github/GoogleContainerTools/skaffold/cmd/skaffold/skaffold.go&quot;\n+ Â Â Â Â Â Â Â Â Â :program &quot;/var/home/shanemcd/github/GoogleContainerTools/skaffold/cmd/skaffold/skaffold.go&quot;\nÂ Â Â Â Â Â Â Â Â Â :args [&quot;build&quot;]))))))\nAnd when launching emacs:\n$ env HOME=/var/home/shanemcd emacs\n\nSeeking a more long-term solution, I was able to add this to my emacs configuration, which handles setting HOME correctly whenever it is symlinked:\n(when-let ((real-home (file-truename (getenv &quot;HOME&quot;))))\n  (when (not (string= (getenv &quot;HOME&quot;) real-home))\n    (setenv &quot;HOME&quot; real-home)))\nI still have a lot more to learn, but with this, I am at least able to set breakpoints in my Golang application, step through the code, and inspect variables. I hope this blog post proves to be useful for others, but if nothing else I will be able to copy + paste these configs the next time I lose them. ğŸ™‚"},"posts/07-gpu-passthrough-journey":{"slug":"posts/07-gpu-passthrough-journey","filePath":"posts/07-gpu-passthrough-journey.md","title":"GPU passthrough with libvirt on Fedora Kinoite","links":[],"tags":[],"content":"I spent the last few days getting GPU passthrough working for my VMs. The goal was to experiment with running LLMs and other GPU workloads in isolated environments. I ran into kernel panics, hanging virsh commands, and display manager crashes along the way. Hereâ€™s what I learned.\nMy setup\n\nFedora Kinoite (ostree/bootc-based system)\nNVIDIA RTX 4070 Ti SUPER (discrete GPU)\nIntel UHD Graphics 770 (integrated GPU)\nlibvirt + QEMU for virtualization\n\nUnderstanding GPU passthrough\nGPU passthrough lets you assign a physical GPU directly to a VM. The VM sees the actual hardware and can install native drivers. This gives near-native performance for GPU workloads.\nThe basic flow:\n\nVM starts â†’ unbind GPU from host driver â†’ bind to vfio-pci â†’ pass to VM\nVM stops â†’ unbind from vfio-pci â†’ rebind to host driver\n\nThe challenges are around timing, driver state, and making sure nothing else is using the GPU during the transition.\nFinding your GPU details\nFirst, find the PCI address of your GPU:\n$ lspci | grep -i nvidia\n01:00.0 VGA compatible controller: NVIDIA Corporation AD103 [GeForce RTX 4070 Ti SUPER] (rev a1)\n01:00.1 Audio device: NVIDIA Corporation AD103 High Definition Audio Controller (rev a1)\nThe PCI address is 01:00.0 for the GPU and 01:00.1 for the audio controller. In sysfs format, these become 0000:01:00.0 and 0000:01:00.1 (add the domain prefix 0000:).\nNext, find the vendor and device IDs:\n$ lspci -n -s 01:00.0\n01:00.0 0300: 10de:2705 (rev a1)\nThe format is class: vendor:device. The vendor ID is 10de (NVIDIA) and the device ID is 2705 (this specific GPU model). Youâ€™ll use this as 10de 2705 (space-separated) when binding drivers.\nPrerequisites\nBefore GPU passthrough will work:\n\n\nIOMMU enabled in kernel boot parameters:\nintel_iommu=on iommu=pt\n\n\n\nSecure Boot disabled in the VM (NVIDIA drivers arenâ€™t signed for secure boot in Linux guests)\n\n\nDual-GPU recommended: Having both integrated and discrete GPUs makes this much easier. Enable integrated graphics in BIOS (look for â€œPrimary Displayâ€, â€œiGPU Multi-Monitorâ€, or â€œIntegrated Graphicsâ€).\n$ lspci | grep -i VGA\n00:02.0 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770]\n01:00.0 VGA compatible controller: NVIDIA Corporation AD103 [GeForce RTX 4070 Ti SUPER]\nWith dual-GPU, the host uses Intel iGPU for display while NVIDIA goes to the VM. Single-GPU passthrough is possible but your host display goes completely black while the VM runs.\n\n\nThe problems I ran into\nProblem 1: virsh nodedev commands hang\nMy initial hooks used virsh nodedev-detach and virsh nodedev-reattach. These would hang indefinitely with no error messages when the system was in certain states.\nSolution: Use direct sysfs manipulation instead. Itâ€™s more reliable and gives immediate errors.\nProblem 2: Kernel panics\nTrying to unbind the NVIDIA driver while it was in use caused kernel panics:\nnvidia 0000:01:00.0: [drm] drm_WARN_ON(!list_empty(&amp;fb-&gt;filp_head))\nlist_del corruption, ffff8b3442b5d310-&gt;next is LIST_POISON1\n\nThe system would become completely unresponsive.\nSolution: Stop the display manager cleanly before unbinding the GPU driver, even with dual-GPU. This unloads NVIDIA modules properly.\nProblem 3: Display manager crashes\nEven with dual-GPU and SDDM stopped before VM start, sometimes after stopping the VM:\n\nSDDM restarts\nKWin/Plasma tries to initialize all GPUs\nFinds nvidia_drm module loaded\nTries to use the NVIDIA GPU before driver is fully ready\nSession crashes\n\nSolution: Manage module loading/unloading carefully in hooks. Explicitly bind devices after loading modules.\nProblem 4: GPU doesnâ€™t rebind after VM shutdown\nUsing new_id wasnâ€™t enough:\n# Registers the ID but doesn&#039;t bind the device\necho &quot;10de 2705&quot; &gt; /sys/bus/pci/drivers/nvidia/new_id\nThe new_id file tells the driver â€œyou can claim devices with this vendor:device IDâ€, but it doesnâ€™t actually bind any specific device to the driver.\nSolution: Explicitly bind the specific PCI device:\necho &quot;0000:01:00.0&quot; &gt; /sys/bus/pci/drivers/nvidia/bind\nProblem 5: libvirt managed=â€˜yesâ€™ hangs everything\nThis was the root cause of most issues. When you use --hostdev with virt-install, libvirt defaults to managed=&#039;yes&#039;, which means libvirt automatically handles unbinding the device from the host driver and binding it to vfio-pci.\nIn theory, this should work seamlessly. In practice, on my Fedora system with NVIDIA GPU passthrough, it hung every single time. libvirtd would become unresponsive, virsh commands would hang, and eventually zombie processes would pile up.\nSolution: Use managed=&#039;no&#039; and handle driver binding entirely in libvirt hooks.\nThe working solution\nDirect sysfs manipulation\nSince virsh commands kept hanging, I use direct sysfs for all driver operations:\n# Unbind from current driver\necho &quot;0000:01:00.0&quot; &gt; /sys/bus/pci/devices/0000:01:00.0/driver/unbind\n \n# Bind to vfio-pci\nmodprobe vfio-pci\necho &quot;10de 2705&quot; &gt; /sys/bus/pci/drivers/vfio-pci/new_id\n \n# Later, bind back to nvidia\nmodprobe nvidia nvidia_modeset nvidia_drm\necho &quot;0000:01:00.0&quot; &gt; /sys/bus/pci/drivers/nvidia/bind\nThis is more reliable - no hanging commands, immediate errors, works even when libvirt is in a weird state.\nUsing managed=â€˜noâ€™\nThe problem: virt-install doesnâ€™t provide a command-line option to set managed=&#039;no&#039;. You have to either:\n\nManually edit the XML after creation\nUse --print-xml to generate XML, modify it, then define with virsh\n\nI chose option 2 and automated it with Ansible.\nAnsible automation\nI created an Ansible role that:\n\nGenerates VM XML using virt-install --print-xml (doesnâ€™t actually create the VM)\nModifies the XML to set managed=&#039;no&#039; on hostdev elements\nDefines and starts the VM with virsh\n\nThe key task:\n- name: Set hostdev managed=&#039;no&#039; in XML for GPU passthrough\n  community.general.xml:\n    path: &quot;/tmp/{{ virt_install_vm_name }}.xml&quot;\n    xpath: &quot;//hostdev[@mode=&#039;subsystem&#039;][@type=&#039;pci&#039;]&quot;\n    attribute: &quot;managed&quot;\n    value: &quot;no&quot;\n  when: virt_install_gpu_passthrough | default(false) | bool\nThis completely avoids libvirtâ€™s managed mode and all the hanging issues.\nAuto-detecting GPU addresses\nInstead of hardcoding PCI addresses, the hooks and Makefile auto-detect the NVIDIA GPU:\n# In the hooks\nNVIDIA_VGA=$(lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;VGA&quot; | awk &#039;{print $1}&#039;)\nNVIDIA_AUDIO=$(lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;Audio&quot; | awk &#039;{print $1}&#039;)\n \n# Get vendor:device IDs\nNVIDIA_VGA_ID=$(lspci -n -s &quot;$NVIDIA_VGA&quot; | awk &#039;{print $3}&#039;)\nNVIDIA_VENDOR=$(echo &quot;$NVIDIA_VGA_ID&quot; | cut -d: -f1)\nNVIDIA_DEVICE=$(echo &quot;$NVIDIA_VGA_ID&quot; | cut -d: -f2)\nNote: Using two separate grep commands works better than grep -i &quot;NVIDIA.*VGA&quot; because lspci output has â€œCorporationâ€ between â€œNVIDIAâ€ and â€œVGAâ€.\nIn the Makefile:\nifeq ($(GPU_PASSTHROUGH),yes)\n  # Auto-detect NVIDIA GPU PCI addresses\n  NVIDIA_VGA_ADDR := $(shell lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;VGA&quot; | awk &#039;{print $$1}&#039;)\n  NVIDIA_AUDIO_ADDR := $(shell lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;Audio&quot; | awk &#039;{print $$1}&#039;)\n  # Error if no NVIDIA GPU found\n  ifeq ($(NVIDIA_VGA_ADDR),)\n    $(error GPU_PASSTHROUGH=yes but no NVIDIA VGA device found. Check &#039;lspci | grep -i nvidia&#039;)\n  endif\nendif\nNo more hardcoded addresses - works on any system with an NVIDIA GPU.\nThe libvirt hooks\nWith managed=&#039;no&#039;, hooks handle ALL driver binding. They auto-detect GPU addresses and handle single vs dual-GPU configurations.\nvfio-startup.sh - When VM starts:\n#!/bin/bash\nset -x\n \n# Auto-detect NVIDIA GPU\nNVIDIA_VGA=$(lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;VGA&quot; | awk &#039;{print $1}&#039;)\nNVIDIA_AUDIO=$(lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;Audio&quot; | awk &#039;{print $1}&#039;)\n \n# Get vendor:device IDs\nNVIDIA_VGA_ID=$(lspci -n -s &quot;$NVIDIA_VGA&quot; | awk &#039;{print $3}&#039;)\nNVIDIA_VENDOR=$(echo &quot;$NVIDIA_VGA_ID&quot; | cut -d: -f1)\nNVIDIA_DEVICE=$(echo &quot;$NVIDIA_VGA_ID&quot; | cut -d: -f2)\n \n# Count GPUs to detect dual-GPU mode\nVGA_COUNT=$(lspci | grep -i &quot;VGA compatible controller&quot; | wc -l)\n \n# Stop SDDM to cleanly unload nvidia\nsystemctl stop sddm.service\nsleep 2\n \n# Unbind GPU from nvidia driver\necho &quot;$NVIDIA_VGA&quot; &gt; /sys/bus/pci/devices/$NVIDIA_VGA/driver/unbind 2&gt;/dev/null || true\nif [ -n &quot;$NVIDIA_AUDIO&quot; ]; then\n    echo &quot;$NVIDIA_AUDIO&quot; &gt; /sys/bus/pci/devices/$NVIDIA_AUDIO/driver/unbind 2&gt;/dev/null || true\nfi\n \n# Unload NVIDIA modules\nmodprobe -r nvidia_drm nvidia_modeset nvidia_uvm nvidia i2c_nvidia_gpu 2&gt;/dev/null || true\n \n# Load vfio-pci and bind GPU\nmodprobe vfio-pci\necho &quot;$NVIDIA_VENDOR $NVIDIA_DEVICE&quot; &gt; /sys/bus/pci/drivers/vfio-pci/new_id 2&gt;/dev/null || true\nif [ -n &quot;$NVIDIA_AUDIO&quot; ]; then\n    NVIDIA_AUDIO_DEVICE=$(echo &quot;$(lspci -n -s &quot;$NVIDIA_AUDIO&quot; | awk &#039;{print $3}&#039;)&quot; | cut -d: -f2)\n    echo &quot;$NVIDIA_VENDOR $NVIDIA_AUDIO_DEVICE&quot; &gt; /sys/bus/pci/drivers/vfio-pci/new_id 2&gt;/dev/null || true\nfi\n \n# If dual-GPU, restart SDDM on iGPU\nif [ &quot;$VGA_COUNT&quot; -gt 1 ]; then\n    systemctl start sddm.service\nfi\nvfio-teardown.sh - When VM stops:\n#!/bin/bash\nset -x\n \n# Auto-detect NVIDIA GPU\nNVIDIA_VGA=$(lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;VGA&quot; | awk &#039;{print $1}&#039;)\nNVIDIA_AUDIO=$(lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;Audio&quot; | awk &#039;{print $1}&#039;)\n \n# Get vendor:device IDs\nNVIDIA_VGA_ID=$(lspci -n -s &quot;$NVIDIA_VGA&quot; | awk &#039;{print $3}&#039;)\nNVIDIA_VENDOR=$(echo &quot;$NVIDIA_VGA_ID&quot; | cut -d: -f1)\nNVIDIA_DEVICE=$(echo &quot;$NVIDIA_VGA_ID&quot; | cut -d: -f2)\n \nVGA_COUNT=$(lspci | grep -i &quot;VGA compatible controller&quot; | wc -l)\n \n# Remove GPU from vfio-pci\necho &quot;$NVIDIA_VENDOR $NVIDIA_DEVICE&quot; &gt; /sys/bus/pci/drivers/vfio-pci/remove_id 2&gt;/dev/null || true\nif [ -n &quot;$NVIDIA_AUDIO&quot; ]; then\n    NVIDIA_AUDIO_DEVICE=$(echo &quot;$(lspci -n -s &quot;$NVIDIA_AUDIO&quot; | awk &#039;{print $3}&#039;)&quot; | cut -d: -f2)\n    echo &quot;$NVIDIA_VENDOR $NVIDIA_AUDIO_DEVICE&quot; &gt; /sys/bus/pci/drivers/vfio-pci/remove_id 2&gt;/dev/null || true\nfi\n \necho &quot;$NVIDIA_VGA&quot; &gt; /sys/bus/pci/drivers/vfio-pci/unbind 2&gt;/dev/null || true\nif [ -n &quot;$NVIDIA_AUDIO&quot; ]; then\n    echo &quot;$NVIDIA_AUDIO&quot; &gt; /sys/bus/pci/drivers/vfio-pci/unbind 2&gt;/dev/null || true\nfi\n \n# Unload vfio-pci\nmodprobe -r vfio-pci 2&gt;/dev/null || true\n \n# Load nvidia modules and bind GPU\nmodprobe nvidia nvidia_modeset nvidia_drm nvidia_uvm i2c_nvidia_gpu\necho &quot;$NVIDIA_VGA&quot; &gt; /sys/bus/pci/drivers/nvidia/bind 2&gt;/dev/null || true\nif [ -n &quot;$NVIDIA_AUDIO&quot; ]; then\n    echo &quot;$NVIDIA_AUDIO&quot; &gt; /sys/bus/pci/drivers/snd_hda_intel/bind 2&gt;/dev/null || true\nfi\n \nsleep 2\n \n# Only restart SDDM if single-GPU (it was stopped during startup)\nif [ &quot;$VGA_COUNT&quot; -eq 1 ]; then\n    # Rebind framebuffer and consoles\n    echo efi-framebuffer.0 &gt; /sys/bus/platform/drivers/efi-framebuffer/bind 2&gt;/dev/null || true\n    echo 1 &gt; /sys/class/vtconsole/vtcon0/bind 2&gt;/dev/null || true\n    echo 1 &gt; /sys/class/vtconsole/vtcon1/bind 2&gt;/dev/null || true\n    sleep 2\n    systemctl start sddm.service\nfi\nqemu - Main hook file that calls startup/teardown:\n#!/bin/bash\n \nGUEST_NAME=&quot;$1&quot;\nOPERATION=&quot;$2&quot;\n \n# Only run for VMs with &quot;-gpu&quot; suffix\nif [[ &quot;$GUEST_NAME&quot; != *-gpu ]]; then\n    exit 0\nfi\n \nHOOK_DIR=&quot;$(dirname &quot;$0&quot;)&quot;\n \nif [ &quot;$OPERATION&quot; = &quot;prepare&quot; ] || [ &quot;$OPERATION&quot; = &quot;start&quot; ]; then\n    &quot;$HOOK_DIR/vfio-startup.sh&quot;\nelif [ &quot;$OPERATION&quot; = &quot;release&quot; ] || [ &quot;$OPERATION&quot; = &quot;stopped&quot; ]; then\n    &quot;$HOOK_DIR/vfio-teardown.sh&quot;\nfi\nMaking it optional\nUsing the VM name as a signal: VMs with -gpu suffix get GPU passthrough, others donâ€™t.\nIn the Makefile:\nGPU_PASSTHROUGH ?= no\n \nifeq ($(GPU_PASSTHROUGH),yes)\n  VM_NAME_FULL := $(VM_NAME)-gpu\n  NVIDIA_VGA_ADDR := $(shell lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;VGA&quot; | awk &#039;{print $$1}&#039;)\n  NVIDIA_AUDIO_ADDR := $(shell lspci -D | grep -i &quot;NVIDIA&quot; | grep -i &quot;Audio&quot; | awk &#039;{print $$1}&#039;)\nelse\n  VM_NAME_FULL := $(VM_NAME)\nendif\nNow I can choose at VM creation time:\n# Without GPU passthrough\nmake virt-install\n \n# With GPU passthrough\nGPU_PASSTHROUGH=yes make virt-install\nThe Makefile passes the GPU addresses to Ansible, which builds the VM XML with hostdev entries and managed=&#039;no&#039;. The hooks detect the -gpu suffix and handle driver binding.\nKeeping SPICE graphics\nYou can have both a virtual display (SPICE/QXL) AND the passed-through NVIDIA GPU. The VM sees both displays.\nvirt-install \\\n  --graphics spice \\\n  --video qxl \\\n  --hostdev 0000:01:00.0 \\\n  --hostdev 0000:01:00.1 \\\n  --boot uefi,firmware.feature0.name=secure-boot,firmware.feature0.enabled=no\nThis gives you:\n\nSPICE access via virt-viewer (for remote management)\nPhysical monitor output via NVIDIA GPU (for primary use)\nFlexibility to use either display\n\nThe complete workflow\n# Create VM with GPU passthrough\nGPU_PASSTHROUGH=yes make virt-install\nThis:\n\nAuto-detects NVIDIA GPU addresses\nRuns Ansible playbook to generate XML with managed=&#039;no&#039;\nDefines and starts the VM\nHooks handle driver binding when VM starts/stops\n\nStarting a VM with GPU passthrough:\n\nHook detects dual-GPU mode\nStops SDDM\nUnbinds NVIDIA GPU from nvidia driver\nUnloads all NVIDIA modules\nLoads vfio-pci and binds GPU to it\nRestarts SDDM on iGPU (dual-GPU only)\nVM starts with full GPU access\n\nStopping the VM:\n\nVM releases GPU\nHook unbinds from vfio-pci\nLoads nvidia modules\nExplicitly binds GPU to nvidia driver\nNVIDIA GPU is available on host again\nRestarts SDDM (single-GPU only)\n\nResults\nAfter implementing the managed=&#039;no&#039; approach:\n\nâœ… VM creation completes without hanging\nâœ… libvirtd stays responsive\nâœ… GPU switches between host and VM seamlessly\nâœ… Display manager handles transitions cleanly\nâœ… No hardcoded PCI addresses\nâœ… No manual XML editing required\nâœ… Can toggle GPU passthrough with a single environment variable\n\nTroubleshooting\nGPU wonâ€™t rebind to nvidia after VM shutdown\nIf the GPU is stuck in an unbound state, youâ€™ll see no driver:\n$ lspci -nnk -s 01:00.0\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation AD103 [GeForce RTX 4070 Ti SUPER] [10de:2705] (rev a1)\n\tSubsystem: PNY Device [196e:141c]\n\tKernel modules: nouveau, nvidia_drm, nvidia\n# Notice: no &quot;Kernel driver in use&quot; line\nThe device is disabled and nvidia wonâ€™t bind:\n$ sudo sh -c &#039;echo &quot;0000:01:00.0&quot; &gt; /sys/bus/pci/drivers/nvidia/bind&#039;\nsh: line 1: echo: write error: No such device\nSolution: Remove and re-scan the PCI device:\nsudo sh -c &#039;echo &quot;1&quot; &gt; /sys/bus/pci/devices/0000:01:00.0/remove&#039;\nsudo sh -c &#039;echo &quot;1&quot; &gt; /sys/bus/pci/rescan&#039;\nsleep 2\nlspci -nnk -s 01:00.0  # Verify it&#039;s bound to nvidia\nlibvirtd hangs or becomes unresponsive\nUsually because:\n\nGPU is in a bad state (not bound to any driver)\nHooks ran but VM never started\nlibvirt is waiting for stuck resources\n\nSolution:\n\nCancel hung commands (Ctrl+C)\nRun teardown hook manually: sudo /etc/libvirt/hooks/vfio-teardown.sh\nIf GPU still wonâ€™t bind, use PCI remove/rescan above\nKill zombie processes: sudo killall -9 libvirtd\nRestart: sudo systemctl restart libvirtd\n\nKey lessons\n\nlibvirtâ€™s managed mode doesnâ€™t work reliably with NVIDIA GPU passthrough on my system - use managed=&#039;no&#039;\nDirect sysfs manipulation is more reliable than virsh commands\nDisplay manager must be stopped before unbinding GPU drivers to avoid kernel panics\nModule loading order matters - canâ€™t unload nvidia_drm if nvidia is loaded\nnew_id registers but doesnâ€™t bind - explicit bind is required\nAuto-detection makes the setup portable across systems\nVM naming convention (suffix) is better than flag files for conditional behavior\n\nThe hooks are in my toolbox repo for reference."},"posts/08-jetkvm-display-fix":{"slug":"posts/08-jetkvm-display-fix","filePath":"posts/08-jetkvm-display-fix.md","title":"Dealing with dual-GPU display issues on KDE Plasma","links":["posts/04-jetkvm-tailscale","posts/07-gpu-passthrough-journey"],"tags":[],"content":"A few months ago I wrote about Getting Tailscale to work on my JetKVM. I had been using it as a remote desktop solution by connecting it to the HDMI port on my NVIDIA GPU. It worked fine, but when I started working on GPU passthrough, I realized Iâ€™d lose all graphics when passing the NVIDIA GPU to a VM.\nThe solution was to enable the integrated graphics in my BIOS and move the JetKVM to the motherboardâ€™s HDMI port. This way I could still access the system remotely (and reach BIOS) even when the NVIDIA GPU was assigned to a VM.\nWhat I didnâ€™t anticipate was that having displays connected to both GPUs would cause some strange issues with KDE Plasma.\nMy setup\n\nFedora Kinoite (Wayland + Plasma 6)\nNVIDIA RTX 4070 Ti SUPER (discrete GPU)\nIntel UHD Graphics 770 (integrated GPU)\nSamsung Odyssey G70NC (4K @ 144 Hz) connected to NVIDIA DisplayPort\nJetKVM connected to motherboardâ€™s HDMI port (Intel iGPU)\n\nThe JetKVM lets you choose from a selection of different EDID options. Iâ€™m using â€œDELL D2721H, 1920x1080â€ which has worked well for my setup.\nThe problems\nAfter moving the JetKVM to the Intel iGPUâ€™s HDMI port, I started running into two separate issues.\nProblem 1: DisplayPort wonâ€™t wake up\nMy primary monitor started having issues after waking from sleep:\n\nTurn on briefly, then go black and repeat this cycle indefinitely\nWake up stuck at 640Ã—480 resolution\nLose all refresh rate options above 60 Hz\n\nThis was frustrating because nothing about the monitor, cable, or GPU had changed. The first few times it happened I rebooted and everything went back to normal, but I knew something was wrong with the display detection.\nTroubleshooting\nThe first thing I checked was whether this was a hardware issue. I looked at /sys/class/drm/card0-HDMI-A-2/modes to see what display modes the kernel was detecting:\n3840x2160\n2560x1440\n1920x1080\n...\n640x480\n\nWhen things were working correctly, this file would have entries like 3840x2160@144 and other high refresh rate modes. When broken, all the high refresh modes were justâ€¦ gone.\nThis told me the EDID wasnâ€™t being read correctly - the driver was falling back to some safe default mode list.\nI tried a bunch of things that didnâ€™t work:\n\nSwitching between different resolution/refresh rate combinations in Plasmaâ€™s settings\nUnplugging and replugging the monitor\nDisabling and re-enabling displays in kscreen-doctor\n\nSometimes switching from 4K@60 to 4K@120 would â€œkickâ€ things back into working, but it was unreliable.\nAfter digging through forums and bug reports, I realized this was a DisplayPort link training issue. When waking from sleep, the DisplayPort connection wasnâ€™t renegotiating properly - the EDID read would fail and the driver would fall back to safe modes like 640Ã—480.\nI found that forcing a known stable mode would reset the stuck state:\nkscreen-doctor output.2.mode.3840x2160@60\nsleep 2\nkscreen-doctor output.2.mode.3840x2160@143.99\nThis worked as a workaround, but I shouldnâ€™t have to run commands manually every time the monitor wakes up.\nEventually I just switched to an HDMI 2.1 cable and the wake issues disappeared entirely. HDMI 2.1 handles the link negotiation much more reliably than DisplayPort on my setup.\nProblem 2: DPI scaling issues\nAfter switching to HDMI, I noticed something else: my Samsung display lookedâ€¦ off. Text seemed larger than it should be, and when I checked the display settings, I realized KDE was scaling everything to match the DPI across both displays.\nThe JetKVM at 1080p has a much lower DPI than my 4K Samsung. KDE was using the lower DPI as the baseline, which made everything on my Samsung look less sharp than it should.\nThe solution\nI didnâ€™t need both displays active all the time. The JetKVM is only useful when I need remote access to the system or BIOS. The rest of the time, I want just my Samsung display running at full quality.\nWhat I needed was a way to easily toggle the JetKVM display on and off from SSH.\nThe script\nI put together a script that auto-detects the JetKVM by reading EDID data from /sys/class/drm/ and uses kscreen-doctor to toggle it.\nThe tricky part is that kscreen-doctor needs access to the Wayland session, which isnâ€™t normally available over SSH or a TTY. The script handles this by setting up the necessary environment variables to communicate with the running Plasma session.\n#!/usr/bin/env bash\n \n# jetkvm-display: enable/disable the JetKVM HDMI output in a KDE Wayland session.\n# Usage: jetkvm-display enable\n#        jetkvm-display disable\n \nset -euo pipefail\n \n# Unique EDID substring for the JetKVM monitor\nJETKVM_EDID_TAG=&quot;DELL D2721H&quot;\n \nexport XDG_RUNTIME_DIR=&quot;/run/user/$(id -u)&quot;\nexport DBUS_SESSION_BUS_ADDRESS=&quot;unix:path=/run/user/$(id -u)/bus&quot;\nexport QT_QPA_PLATFORM=&quot;wayland&quot;\n \n# Make sure sudo can prompt once if needed (for reading EDID)\nsudo -v &gt;/dev/null 2&gt;&amp;1 || true\n \n# Run kscreen-doctor inside the active KDE Wayland session\nks_doctor() {\n  kscreen-doctor &quot;$@&quot;\n}\n \n# Find the HDMI connector whose EDID contains the JetKVM tag\nfind_jetkvm_connector() {\n  local f basename connector\n  for f in /sys/class/drm/*HDMI-A-*/edid; do\n    [ -r &quot;$f&quot; ] || continue\n    if sudo strings &quot;$f&quot; 2&gt;/dev/null | grep -q &quot;${JETKVM_EDID_TAG}&quot;; then\n      basename=&quot;$(basename &quot;$(dirname &quot;$f&quot;)&quot;)&quot;   # e.g. card0-HDMI-A-1\n      connector=&quot;${basename#*-}&quot;                 # strip &quot;card0-&quot; -&gt; HDMI-A-1\n      echo &quot;${connector}&quot;\n      return 0\n    fi\n  done\n  return 1\n}\n \n# Verify that kscreen-doctor knows about this connector name\ndetect_jetkvm_output_name() {\n  local connector=&quot;$(\n    find_jetkvm_connector || {\n      echo &quot;jetkvm-display: EDID tag &#039;${JETKVM_EDID_TAG}&#039; not found on any HDMI connector&quot; &gt;&amp;2\n      return 1\n    }\n  )&quot;\n \n  # Check that kscreen-doctor -o lists an Output with that name\n  if ! ks_doctor -o | awk -v conn=&quot;$connector&quot; &#039;\n      /Output:/ {\n        # &quot;Output: 2 HDMI-A-1 &lt;uuid&gt;&quot;\n        name = $3\n        if (name == conn) {\n          found = 1\n        }\n      }\n      END { exit (!found) }\n    &#039;; then\n    echo &quot;jetkvm-display: found connector &#039;${connector}&#039; for JetKVM, but no matching Output name in kscreen-doctor -o&quot; &gt;&amp;2\n    echo &quot;jetkvm-display: Outputs seen:&quot; &gt;&amp;2\n    ks_doctor -o | grep &#039;Output:&#039; &gt;&amp;2 || true\n    return 1\n  fi\n \n  echo &quot;jetkvm-display: JetKVM detected on connector &#039;${connector}&#039;&quot; &gt;&amp;2\n  echo &quot;${connector}&quot;\n}\n \nJET_OUTPUT_NAME=&quot;$(detect_jetkvm_output_name || true)&quot;\n \nif [ -z &quot;${JET_OUTPUT_NAME}&quot; ]; then\n  echo &quot;jetkvm-display: could not auto-detect JetKVM&quot; &gt;&amp;2\n  exit 1\nfi\n \ndo_enable() {\n  ks_doctor &quot;output.${JET_OUTPUT_NAME}.enable&quot;\n}\n \ndo_disable() {\n  ks_doctor &quot;output.${JET_OUTPUT_NAME}.disable&quot;\n}\n \ncase &quot;${1:-}&quot; in\n  enable)\n    do_enable\n    ;;\n  disable)\n    do_disable\n    ;;\n  *)\n    echo &quot;Usage: $0 {enable|disable}&quot; &gt;&amp;2\n    exit 1\n    ;;\nesac\nI saved this to ~/.local/bin/jetkvm-display and made it executable.\nNow when I need remote access:\nssh desktop\njetkvm-display enable\n# Access the system through JetKVM\njetkvm-display disable\nThe key things this script does:\n\nReads EDID data from /sys/class/drm/ to find which HDMI connector has the JetKVM (searches for â€œDELL D2721Hâ€)\nVerifies that kscreen-doctor knows about that connector\nSets up the necessary environment variables (XDG_RUNTIME_DIR, WAYLAND_DISPLAY, DBUS_SESSION_BUS_ADDRESS) to talk to the Wayland session from SSH\nUses kscreen-doctor to cleanly enable/disable the display\n\nResults\nWith the JetKVM display disabled by default:\n\nNo wake issues (after switching to HDMI 2.1)\nNo DPI scaling problems\nSamsung runs at full 4K @ 143.99 Hz\nI can still enable the JetKVM whenever I need remote access\n\nSwitching from DisplayPort to HDMI 2.1 completely resolved the wake issues. Combined with keeping the JetKVM display disabled when not in use, everything has been stable since."},"posts/09-auto-syncing-terminal-themes":{"slug":"posts/09-auto-syncing-terminal-themes","filePath":"posts/09-auto-syncing-terminal-themes.md","title":"Auto-switching Konsole themes with tinty and KDE light/dark mode","links":[],"tags":[],"content":"Iâ€™ve spent years cycling through color schemes in my terminal and text editor - installing them, using them for a few days, then switching to something else. The Tomorrow theme was the first one that stuck. Its author later built Base16, a system for generating consistent color schemes across different applications. While Base16 hasnâ€™t been actively maintained in recent years, the concept stuck around.\nI recently discovered tinty, a maintained Base16 theme manager that applies color schemes to your terminal using escape sequences. It can switch themes on the fly without restarting the terminal.\nBefore I could use tinty with Konsole, I needed to add support for it. The tinted-terminal project generates terminal color schemes for various emulators, but Konsole was missing. I sent a PR which was quickly merged (thanks!).\nWith Konsole support in place, the next problem was making it automatic. KDE Plasma 6.5 recently added automatic light/dark mode switching based on time of day, but terminal color schemes donâ€™t follow along. You can manually switch them, but that defeats the purpose.\nBuilding the plugin\nI wanted a zsh plugin that would:\n\nDetect when the desktop switches between light/dark mode\nApply the appropriate tinty theme automatically\nUpdate all open terminal tabs, not just one\n\nDetecting theme changes\nModern desktops expose theme settings through the XDG Desktop Portal over D-Bus. The org.freedesktop.appearance interface has a color-scheme setting that returns:\n\n0 - No preference (treat as light)\n1 - Dark\n2 - Light\n\nYou can query it with dbus-send:\ndbus-send --session --print-reply --dest=org.freedesktop.portal.Desktop \\\n  /org/freedesktop/portal/desktop \\\n  org.freedesktop.portal.Settings.Read \\\n  string:&#039;org.freedesktop.appearance&#039; \\\n  string:&#039;color-scheme&#039;\nAnd monitor changes with dbus-monitor:\ndbus-monitor --session \\\n  &quot;type=&#039;signal&#039;,interface=&#039;org.freedesktop.portal.Settings&#039;,member=&#039;SettingChanged&#039;,arg0=&#039;org.freedesktop.appearance&#039;,arg1=&#039;color-scheme&#039;&quot;\nThe broadcasting problem\nThe first version worked for new tabs but failed when switching themes. Running tinty apply from a background job only updated whichever tab the job happened to be associated with. The other tabs stayed on the old theme.\nI tried several approaches:\n\nWriting to parent process file descriptors (/proc/$PPID/fd/1) - permission denied\nUsing a queue file and precmd hooks - timing issues with initial theme on new tabs\nBroadcasting to all /dev/pts/* devices - triggered desktop notifications from KDE daemons\n\nThe solution: shell registration\nEach shell that loads the plugin registers itself by writing its PID to /tmp/tinty-shells/&lt;pts-number&gt;:\nlocal my_tty=$(_tinty_get_tty)\nlocal my_pts_num=&quot;&quot;\n[[ &quot;$my_tty&quot; =~ /dev/pts/([0-9]+)$ ]] &amp;&amp; my_pts_num=&quot;${match[1]}&quot;\n \nif [[ -n &quot;$my_pts_num&quot; ]]; then\n  mkdir -p /tmp/tinty-shells\n  echo $$ &gt; &quot;/tmp/tinty-shells/$my_pts_num&quot;\nfi\nWhen a theme change is detected, the plugin:\n\nAcquires a lock (so only one tab does the work)\nRuns tinty apply once and captures the output\nWrites the escape sequences to each registered terminal device\n\n_tinty_apply_for_scheme() {\n  local color_scheme=$1\n \n  {\n    flock -n 9 || exit 0  # Skip if another tab is applying\n \n    local theme=$(_tinty_theme_for_scheme &quot;$color_scheme&quot;)\n    local tinty_output=$($TINTY_BIN apply &quot;$theme&quot; 2&gt;/dev/null)\n \n    [[ -d /tmp/tinty-shells ]] || exit 0\n    for pts_file in /tmp/tinty-shells/*; do\n      [[ -e &quot;$pts_file&quot; ]] || continue\n \n      local pts=&quot;/dev/pts/$(basename &quot;$pts_file&quot;)&quot;\n      local pid=$(cat &quot;$pts_file&quot; 2&gt;/dev/null)\n \n      # Verify shell is running and terminal is writable\n      if [[ -n &quot;$pid&quot; ]] &amp;&amp; kill -0 &quot;$pid&quot; 2&gt;/dev/null &amp;&amp; [[ -w &quot;$pts&quot; ]]; then\n        printf &#039;%s&#039; &quot;$tinty_output&quot; &gt; &quot;$pts&quot; 2&gt;/dev/null\n      else\n        rm -f &quot;$pts_file&quot;  # Clean up stale registration\n      fi\n    done\n  } 9&gt;/tmp/tinty-portal.lock\n}\nThis way:\n\ntinty apply runs once, not once per tab\nOnly registered shells (running this plugin) get updated\nStale registrations are cleaned up automatically\nLock prevents race conditions between multiple watchers\n\nZLE-safe initialization\nRunning the D-Bus watcher immediately on plugin load caused issues with cursor positioning and widgets. The solution was to defer initialization until ZLE is ready:\nautoload -Uz add-zle-hook-widget\n \ntinty_portal_zle_init() {\n  [[ -n &quot;$TINTY_PORTAL_WATCHER_RUNNING&quot; ]] &amp;&amp; return 0\n  export TINTY_PORTAL_WATCHER_RUNNING=1\n \n  add-zle-hook-widget -d zle-line-init tinty_portal_zle_init\n  # ... start watcher\n}\n \nadd-zle-hook-widget zle-line-init tinty_portal_zle_init\nThis ensures the watcher starts only after the prompt is ready and widgets are stable.\nInstallation\nClone the plugin to your oh-my-zsh custom plugins directory:\ngit clone github.com/shanemcd/zsh-auto-tinty \\\n  ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/plugins/auto-tinty\nConfigure your light and dark themes in ~/.zshrc:\nexport ZSH_TINTY_LIGHT=&quot;base16-ia-light&quot;\nexport ZSH_TINTY_DARK=&quot;base16-ia-dark&quot;\nplugins+=(auto-tinty)\nReload your shell:\nexec zsh\nHow it works\nWhen you open a new terminal tab:\n\nPlugin loads and registers the shell in /tmp/tinty-shells/\nQueries current theme via D-Bus\nApplies the appropriate tinty theme directly to that terminal\nStarts a dbus-monitor background job (once per shell)\nOn shell exit, cleans up registration and kills the watcher\n\nWhen the desktop theme changes:\n\nOne of the D-Bus watchers detects the signal\nWaits 200ms for signals to settle (debouncing)\nAcquires lock in /tmp/tinty-portal.lock\nRuns tinty apply once\nBroadcasts escape sequences to all registered terminals\nReleases lock\n\nAll open terminal tabs switch themes simultaneously.\nResults\nNow when my desktop switches to light mode in the morning, all my terminal tabs follow along. When it switches back to dark mode in the evening, same thing.\nNo manual theme switching, no forgetting to update that one terminal tab you opened three days ago.\nThe plugin is at github.com/shanemcd/zsh-auto-tinty. It should work with any terminal that supports tintyâ€™s escape sequences and any desktop that implements the XDG Desktop Portal. If you run into problems or have improvements, please open an issue or send a PR."},"posts/index":{"slug":"posts/index","filePath":"posts/index.md","title":"Posts","links":[],"tags":[],"content":"These are things Iâ€™ve written."}}